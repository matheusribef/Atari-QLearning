{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b6f75d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Jul  3 15:32:19 2021\n",
    "\n",
    "@author: Matheus\n",
    "\"\"\"\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "from keras.optimizers import Adam\n",
    "import random\n",
    "\n",
    "#hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 1\n",
    "epsilon = 0.99\n",
    "epsilon_decay = 0.99\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self, action_space):\n",
    "        #hyperparameters\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        #experience replay\n",
    "        self.memory = []\n",
    "        self.batch_size = 32\n",
    "        \n",
    "        #AI model\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Conv2D(32, (8,8), strides=(4,4), input_shape=(210,160,3)))\n",
    "        self.model.add(Conv2D(64, (4,4), strides=(2,2), activation='relu'))\n",
    "        self.model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(512, activation='relu'))\n",
    "        self.model.add(Dense(256, activation='relu'))\n",
    "        self.model.add(Dense(action_space, activation='linear'))\n",
    "        self.model.compile(loss='mse', optimizer=Adam(learning_rate=self.alpha))\n",
    "        \n",
    "    def update(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        #train the model\n",
    "        mem_sample = random.sample(self.memory, self.batch_size)\n",
    "        for state, action, reward, next_state, done in mem_sample:\n",
    "            q = self.model.predict(state)\n",
    "            q_update = self.alpha * ( reward + self.gamma * np.max(self.model.predict(next_state)) )\n",
    "            q[0, action] = q_update\n",
    "            self.model.fit(state, q, verbose=0)\n",
    "        \n",
    "        #decay epsilon\n",
    "        self.epsilon *= epsilon_decay\n",
    "        \n",
    "    def memory_add(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "        #clear memory\n",
    "        if len(self.memory) > 512:\n",
    "            pct = int(len(self.memory) * 0.4)\n",
    "            self.memory = self.memory[pct:]\n",
    "        \n",
    "    def choose_action(self, state, env):\n",
    "        if random.random() < self.epsilon:\n",
    "            #exploration\n",
    "            return env.action_space.sample()\n",
    "        #exploitation\n",
    "        return np.argmax(self.model.predict(state))\n",
    "    \n",
    "    def save(self):\n",
    "        self.model.save_weights('weights.h5')\n",
    "        \n",
    "    def load(self):\n",
    "        self.model.load_weights('weights.h5')\n",
    "        \n",
    "        \n",
    "def get_state(state):\n",
    "    state = np.array(state) \n",
    "    state = state / 255\n",
    "    state = state.reshape(-1, 210, 160, 3)\n",
    "    return state\n",
    "        \n",
    "def game():\n",
    "    #make environment and model\n",
    "    env = gym.make('Breakout-v0')\n",
    "    dqn = DQN(env.action_space.n)\n",
    "    \n",
    "    #start loop and train\n",
    "    for episode in range(10000):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        dqn.load()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            #sanitize state img\n",
    "            state = get_state(state)\n",
    "            env.render()\n",
    "            \n",
    "            #get random or best action\n",
    "            action = dqn.choose_action(state, env)\n",
    "            \n",
    "            #get next_state afteraction\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "            \n",
    "            #save\n",
    "            dqn.memory_add(state, action, reward, get_state(next_state), done)\n",
    "            \n",
    "            #skip 4 frames\n",
    "            for _ in range(4):\n",
    "                next_state, reward, done, info = env.step(0)\n",
    "                score += reward\n",
    "            \n",
    "            #go to next_state\n",
    "            state = next_state\n",
    "            \n",
    "        #train ai and show score\n",
    "        dqn.update()\n",
    "        dqn.save()\n",
    "        print('Episode', episode, 'score', score)\n",
    "        \n",
    "    #test ai\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    env.render()\n",
    "    while not done:\n",
    "        state = get_state(state)\n",
    "        env.render()\n",
    "        action = dqn.choose_action(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        state = get_state(next_state)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8387906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 score 4.0\n",
      "Episode 1 score 2.0\n",
      "Episode 2 score 1.0\n",
      "Episode 3 score 0.0\n",
      "Episode 4 score 0.0\n",
      "Episode 5 score 1.0\n",
      "Episode 6 score 1.0\n",
      "Episode 7 score 0.0\n",
      "Episode 8 score 1.0\n",
      "Episode 9 score 5.0\n",
      "Episode 10 score 0.0\n",
      "Episode 11 score 1.0\n",
      "Episode 12 score 2.0\n",
      "Episode 13 score 0.0\n",
      "Episode 14 score 1.0\n",
      "Episode 15 score 1.0\n",
      "Episode 16 score 3.0\n",
      "Episode 17 score 1.0\n",
      "Episode 18 score 1.0\n",
      "Episode 19 score 2.0\n",
      "Episode 20 score 0.0\n",
      "Episode 21 score 0.0\n",
      "Episode 22 score 0.0\n",
      "Episode 23 score 0.0\n",
      "Episode 24 score 2.0\n",
      "Episode 25 score 2.0\n",
      "Episode 26 score 2.0\n",
      "Episode 27 score 1.0\n",
      "Episode 28 score 1.0\n",
      "Episode 29 score 2.0\n",
      "Episode 30 score 2.0\n",
      "Episode 31 score 1.0\n",
      "Episode 32 score 2.0\n",
      "Episode 33 score 2.0\n",
      "Episode 34 score 0.0\n",
      "Episode 35 score 5.0\n",
      "Episode 36 score 0.0\n",
      "Episode 37 score 3.0\n",
      "Episode 38 score 2.0\n",
      "Episode 39 score 1.0\n",
      "Episode 40 score 2.0\n",
      "Episode 41 score 3.0\n",
      "Episode 42 score 2.0\n",
      "Episode 43 score 2.0\n",
      "Episode 44 score 0.0\n",
      "Episode 45 score 0.0\n",
      "Episode 46 score 1.0\n",
      "Episode 47 score 4.0\n",
      "Episode 48 score 2.0\n",
      "Episode 49 score 2.0\n",
      "Episode 50 score 2.0\n",
      "Episode 51 score 1.0\n",
      "Episode 52 score 0.0\n",
      "Episode 53 score 1.0\n",
      "Episode 54 score 0.0\n",
      "Episode 55 score 0.0\n",
      "Episode 56 score 4.0\n"
     ]
    }
   ],
   "source": [
    "game()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
